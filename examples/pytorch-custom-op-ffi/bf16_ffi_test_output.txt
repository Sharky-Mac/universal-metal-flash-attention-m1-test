Metal SDPA backend initialized successfully
/Users/kash/src/universal-metal-flash-attention/examples/pytorch-custom-op-ffi/test_bf16_ffi_conversion.py:59: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  print(f"  Storage size: {tensor.storage().size()}")
âœ“ metal_sdpa_extension imported successfully
BF16 FFI Conversion Test Suite
============================================================
PyTorch version: 2.8.0
BFloat16 available: True
Metal extension available: True
MPS backend available: True

============================================================
TEST: Test 1: torch_dtype_to_mfa_dtype Mapping
============================================================

Testing float32 (torch.float32) mapping:
  Created tensor with dtype: torch.float32
  Tensor dtype name: torch.float32
  Output dtype: torch.float32
  Dtype preserved: True
  âœ“ Dtype mapping correct for float32

Testing float16 (torch.float16) mapping:
  Created tensor with dtype: torch.float16
  Tensor dtype name: torch.float16
  Output dtype: torch.float16
  Dtype preserved: True
  âœ“ Dtype mapping correct for float16

Testing bfloat16 (torch.bfloat16) mapping:
  Created tensor with dtype: torch.bfloat16
  Tensor dtype name: torch.bfloat16
  Output dtype: torch.bfloat16
  Dtype preserved: True
  âœ“ Dtype mapping correct for bfloat16
âœ“ PASS: All dtype mappings correct

============================================================
TEST: Test 2: ensure_contiguous_cpu with BF16
============================================================

Testing contiguous_cpu:

Tensor inspection - Input contiguous_cpu:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: cpu
  Is contiguous: True
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [-2.546875, -0.9140625, 0.39453125, -1.6953125, -0.318359375]
  Value range: [-3.078125, 3.203125]
  As FP32 range: [-3.078125, 3.203125]
  Original: dtype=torch.bfloat16, device=cpu, contiguous=True

Tensor inspection - Output contiguous_cpu:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: cpu
  Is contiguous: True
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [-1.4381384971784428e-11, -0.8984375, 0.00194549560546875, -1.6640625, 2.3773197751029943e-34]
  Value range: [nan, nan]
  As FP32 range: [nan, nan]
  Result: dtype_preserved=True, values_reasonable=False
  âœ— contiguous_cpu test

Testing non_contiguous_cpu:

Tensor inspection - Input non_contiguous_cpu:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: cpu
  Is contiguous: False
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [-0.78125, -0.62890625, 0.0625, 0.91796875, 0.224609375]
  Value range: [-2.968750, 3.109375]
  As FP32 range: [-2.968750, 3.109375]
  Original: dtype=torch.bfloat16, device=cpu, contiguous=False

Tensor inspection - Output non_contiguous_cpu:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: cpu
  Is contiguous: False
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [-105381888.0, -0.515625, 1.663157587499437e+34, 0.91015625, -7.2824226022142356e-34]
  Value range: [nan, nan]
  As FP32 range: [nan, nan]
  Result: dtype_preserved=True, values_reasonable=False
  âœ— non_contiguous_cpu test

Testing contiguous_mps:

Tensor inspection - Input contiguous_mps:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: mps:0
  Is contiguous: True
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [-2.0625, -1.140625, 0.291015625, 1.046875, 0.048095703125]
  Value range: [-2.750000, 2.625000]
  As FP32 range: [-2.750000, 2.625000]
  Original: dtype=torch.bfloat16, device=mps:0, contiguous=True

Tensor inspection - Output contiguous_mps:
  Dtype: torch.bfloat16
  Shape: torch.Size([2, 4, 8, 16])
  Device: mps:0
  Is contiguous: True
  Element size: 2 bytes
  Storage size: 1024
  First 5 values: [7.793749371142286e+20, -1.109375, -3.805541685128069e+17, 1.0078125, -1.2229038175983961e-20]
  Value range: [nan, nan]
  As FP32 range: [nan, nan]
  Result: dtype_preserved=True, values_reasonable=False
  âœ— contiguous_mps test
âœ— FAIL: ensure_contiguous_cpu has bf16 issues

============================================================
TEST: Test 3: BF16 Value Corruption Detection
============================================================
Testing with known bf16-representable values:
âœ— FAIL: Exception during value corruption test: The expanded size of the tensor (16) must match the existing size (3) at non-singleton dimension 3.  Target sizes: [1, 4, 8, 16].  Tensor sizes: [1, 1, 2, 3]

============================================================
TEST: Test 4: Direct FFI Dtype Behavior
============================================================
Available extension functions:
  BlockSizeConfig
  HybridGranularityConfig
  HybridStrategy
  MetalSDPABackend
  OutputPrecision
  QuantizationConfig
  QuantizationGranularity
  QuantizationPrecision
  TensorAnalysisMetrics
  get_version
  is_metal_available
  metal_scaled_dot_product_attention
  quantized_scaled_dot_product_attention
  quantized_scaled_dot_product_attention_enhanced
  quantized_scaled_dot_product_attention_unified
  quantized_scaled_dot_product_attention_with_config
  register_backend
  unregister_backend

Testing direct FFI path:
Minimal input shapes: q=torch.Size([1, 1, 1, 2]), k=torch.Size([1, 1, 1, 2]), v=torch.Size([1, 1, 1, 2])
Input dtypes: q=torch.bfloat16, k=torch.bfloat16, v=torch.bfloat16
Input values:
  q: [1.0, 0.5]
  k: [0.5, 1.0]
  v: [0.25, 0.75]

Memory layout before FFI:
  q contiguous: True, storage size: 2
  k contiguous: True, storage size: 2
  v contiguous: True, storage size: 2

FFI call successful!
Output shape: torch.Size([1, 1, 1, 2])
Output dtype: torch.bfloat16
Output values: [0.25, 0.75]
Output contiguous: True

Validation:
  Input dtype preserved: True
  Output dtype correct: True
  Shape preserved: True
  Values finite: True
âœ“ PASS: Direct FFI dtype behavior correct

============================================================
TEST: Test 5: BF16 Tensor Round-trip
============================================================
Original BF16 values: [1.0, -1.0, 0.5, -0.5, 0.25, -0.25, 1.5, -1.5]
BF16 CPU conversion: True
BF16 contiguous conversion: True
BF16 -> FP32 -> BF16 round-trip: True
BF16 memory round-trip failed: Got unsupported ScalarType BFloat16
âœ— FAIL: Some BF16 round-trips failed - potential conversion issues

============================================================
FFI CONVERSION TEST SUMMARY
============================================================
Tests passed: 2/5
âœ— SOME FFI CONVERSION TESTS FAILED

Diagnostic Summary:
1. If torch_dtype_to_mfa_dtype mapping failed:
   -> Check C++ backend torch_dtype_to_mfa_dtype function
   -> Verify MFA_PRECISION_BF16 enum value is correct
2. If ensure_contiguous_cpu failed:
   -> Check ensure_contiguous_cpu function for dtype preservation
   -> Look for inadvertent .to() calls that change dtype
3. If value corruption was detected:
   -> Check Metal kernel implementations for bf16 support
   -> Verify buffer creation preserves bf16 data
4. If direct FFI behavior failed:
   -> Check FFI function signatures and parameter passing
   -> Verify Metal buffer creation with correct precision
5. If round-trip tests failed:
   -> Check for unnecessary dtype conversions in FFI pipeline
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Float -> [2,4,2,4] dtype=Float
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Float -> [2,4,2,4] dtype=Float
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Float -> [2,4,2,4] dtype=Float
ðŸ“Š Regular attention dimensions: batch=2, seq_q=4, seq_kv=4, heads=2, dim=4
ðŸ“‹ Created output tensor: shape=[2,4,2,4] dtype=Float
â„¹ï¸  Q tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  K tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  V tensor is non-contiguous (strides-based view) - using stride-aware buffer
ðŸ“Š Using stride-aware buffers:
  Q shape: [2,4,2,4], strides: [32,4,16,1]
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,4,2,4] dtype=Float -> [2,2,4,4] dtype=Float
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Half -> [2,4,2,4] dtype=Half
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Half -> [2,4,2,4] dtype=Half
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=Half -> [2,4,2,4] dtype=Half
ðŸ“Š Regular attention dimensions: batch=2, seq_q=4, seq_kv=4, heads=2, dim=4
ðŸ“‹ Created output tensor: shape=[2,4,2,4] dtype=Half
â„¹ï¸  Q tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  K tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  V tensor is non-contiguous (strides-based view) - using stride-aware buffer
ðŸ“Š Using stride-aware buffers:
  Q shape: [2,4,2,4], strides: [32,4,16,1]
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,4,2,4] dtype=Half -> [2,2,4,4] dtype=Half
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=BFloat16 -> [2,4,2,4] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=BFloat16 -> [2,4,2,4] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,2,4,4] dtype=BFloat16 -> [2,4,2,4] dtype=BFloat16
ðŸ“Š Regular attention dimensions: batch=2, seq_q=4, seq_kv=4, heads=2, dim=4
ðŸ“‹ Created output tensor: shape=[2,4,2,4] dtype=BFloat16
â„¹ï¸  Q tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  K tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  V tensor is non-contiguous (strides-based view) - using stride-aware buffer
ðŸ“Š Using stride-aware buffers:
  Q shape: [2,4,2,4], strides: [32,4,16,1]
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,4,2,4] dtype=BFloat16 -> [2,2,4,4] dtype=BFloat16
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ“Š Regular attention dimensions: batch=2, seq_q=8, seq_kv=8, heads=4, dim=16
ðŸ“‹ Created output tensor: shape=[2,8,4,16] dtype=BFloat16
â„¹ï¸  Q tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  K tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  V tensor is non-contiguous (strides-based view) - using stride-aware buffer
ðŸ“Š Using stride-aware buffers:
  Q shape: [2,8,4,16], strides: [512,16,128,1]
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,8,4,16] dtype=BFloat16 -> [2,4,8,16] dtype=BFloat16
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ“Š Regular attention dimensions: batch=2, seq_q=8, seq_kv=8, heads=4, dim=16
ðŸ“‹ Created output tensor: shape=[2,8,4,16] dtype=BFloat16
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,8,4,16] dtype=BFloat16 -> [2,4,8,16] dtype=BFloat16
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [2,4,8,16] dtype=BFloat16 -> [2,8,4,16] dtype=BFloat16
ðŸ“Š Regular attention dimensions: batch=2, seq_q=8, seq_kv=8, heads=4, dim=16
ðŸ“‹ Created output tensor: shape=[2,8,4,16] dtype=BFloat16
â„¹ï¸  Q tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  K tensor is non-contiguous (strides-based view) - using stride-aware buffer
â„¹ï¸  V tensor is non-contiguous (strides-based view) - using stride-aware buffer
ðŸ“Š Using stride-aware buffers:
  Q shape: [2,8,4,16], strides: [512,16,128,1]
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [2,8,4,16] dtype=BFloat16 -> [2,4,8,16] dtype=BFloat16
ðŸ“‹ Converting PyTorch layout [B,H,S,D] to Metal layout [B,S,H,D]
ðŸ”„ Converted FLUX->Metal: [1,1,1,2] dtype=BFloat16 -> [1,1,1,2] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [1,1,1,2] dtype=BFloat16 -> [1,1,1,2] dtype=BFloat16
ðŸ”„ Converted FLUX->Metal: [1,1,1,2] dtype=BFloat16 -> [1,1,1,2] dtype=BFloat16
ðŸ“Š Regular attention dimensions: batch=1, seq_q=1, seq_kv=1, heads=1, dim=2
ðŸ“‹ Created output tensor: shape=[1,1,1,2] dtype=BFloat16
ðŸ”„ Converting output from Metal layout [B,S,H,D] back to PyTorch layout [B,H,S,D]
ðŸ”„ Converted Metal->FLUX: [1,1,1,2] dtype=BFloat16 -> [1,1,1,2] dtype=BFloat16
